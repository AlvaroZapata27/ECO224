{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of Lab 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 6\n",
    "Members:\n",
    "1. Eljaer Eusebio\n",
    "2. Claudia Vivas\n",
    "3. Luis Sandoval\n",
    "4. Andre Tapia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdmpy\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simulation Design 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulationg desing in the case B=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "B = 50\n",
    "Naive = np.zeros( B )\n",
    "Orthogonal = np.zeros( B )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range( 0, B ):\n",
    "    n = 100\n",
    "    p = 100\n",
    "    beta = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "    gamma = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "\n",
    "    mean = 0\n",
    "    sd = 1\n",
    "    X = np.random.normal( mean , sd, n * p ).reshape( n, p )\n",
    "\n",
    "    D = ( X @ gamma ) + np.random.normal( mean , sd, n ).reshape( n, 1 )/4 \n",
    "   \n",
    "    # DGP \n",
    "    Y = 5*D + ( X @ beta ) + np.random.normal( mean , sd, n ).reshape( n, 1 )\n",
    "    # single selection method\n",
    "    r_lasso_estimation = hdmpy.rlasso( np.concatenate( ( D , X ) , axis  =  1 ) , Y , post = True ) # Regress main equation by lasso\n",
    "\n",
    "    coef_array = r_lasso_estimation.est[ 'coefficients' ].iloc[ 2:, :].to_numpy()    # Get \"X\" coefficients \n",
    "\n",
    "    SX_IDs = np.where( coef_array != 0 )[0]\n",
    "\n",
    "    # In case all X coefficients are zero, then regress Y on D\n",
    "    if sum(SX_IDs) == 0 : \n",
    "        Naive[ i ] = sm.OLS( Y , sm.add_constant(D) ).fit().summary2().tables[1].round(3).iloc[ 1, 0 ] \n",
    "\n",
    "    # Otherwise, then regress Y on X and D (but only in the selected coefficients)\n",
    "    elif sum( SX_IDs ) > 0 :\n",
    "        X_D = np.concatenate( ( D, X[:, SX_IDs ] ) , axis = 1 )\n",
    "        Naive[ i ] = sm.OLS( Y , sm.add_constant( X_D ) ).fit().summary2().tables[1].round(3).iloc[ 1, 0]\n",
    "\n",
    "    # In both cases we save D coefficient\n",
    "        \n",
    "    # Regress residuals. \n",
    "    resY = hdmpy.rlasso( X , Y , post = False ).est[ 'residuals' ]\n",
    "    resD = hdmpy.rlasso( X , D , post = False ).est[ 'residuals' ]\n",
    "    Orthogonal[ i ] = sm.OLS( resY , sm.add_constant( resD ) ).fit().summary2().tables[1].round(3).iloc[ 1, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Orto_breaks = [-1.2, -1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2]\n",
    "Naive_breaks = [-0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we create a histogram\n",
    "fig, axs = plt.subplots(1, 1, sharex= True, tight_layout=True)\n",
    "\n",
    "N, bins, patches = axs.hist( Orthogonal - 5 , range = (-2, 2), density = True , bins = Orto_breaks)\n",
    "\n",
    "fracs = ((N**(1 / 5)) / N.max())\n",
    "norm = colors.Normalize(fracs.min(), fracs.max())\n",
    " \n",
    "for thisfrac, thispatch in zip(fracs, patches):\n",
    "    color = plt.cm.viridis(norm(thisfrac))\n",
    "    thispatch.set_facecolor(color)\n",
    "\n",
    "plt.title('Orthogonal', size=20)\n",
    "plt.xlabel(\"Difference of Orhtogonal - True\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create histogram\n",
    "fig, axs = plt.subplots(1, 1, sharex= True, tight_layout=True)\n",
    "\n",
    "N, bins, patches = axs.hist( Naive - 5, range = (-2, 2), density = True , bins = Naive_breaks)\n",
    "\n",
    "fracs = ((N**(1 /5 )) / N.max())\n",
    "norm = colors.Normalize(fracs.min(), fracs.max())\n",
    " \n",
    "for thisfrac, thispatch in zip(fracs, patches):\n",
    "    color = plt.cm.viridis(norm(thisfrac))\n",
    "    thispatch.set_facecolor(color)\n",
    "\n",
    "plt.title('Naive', size=20)\n",
    "plt.xlabel(\"Difference of Naive - True\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Simulation design B=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "B = 100\n",
    "Naive = np.zeros( B )\n",
    "Orthogonal = np.zeros( B )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range( 0, B ):\n",
    "    n = 100\n",
    "    p = 100\n",
    "    beta = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "    gamma = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "\n",
    "    mean = 0\n",
    "    sd = 1\n",
    "    X = np.random.normal( mean , sd, n * p ).reshape( n, p )\n",
    "\n",
    "    D = ( X @ gamma ) + np.random.normal( mean , sd, n ).reshape( n, 1 )/4 \n",
    "    \n",
    "    # DGP \n",
    "    Y = 5*D + ( X @ beta ) + np.random.normal( mean , sd, n ).reshape( n, 1 )\n",
    "    # single selection method\n",
    "    r_lasso_estimation = hdmpy.rlasso( np.concatenate( ( D , X ) , axis  =  1 ) , Y , post = True ) # Regress main equation by lasso\n",
    "\n",
    "    coef_array = r_lasso_estimation.est[ 'coefficients' ].iloc[ 2:, :].to_numpy()    # Get \"X\" coefficients \n",
    "\n",
    "    SX_IDs = np.where( coef_array != 0 )[0]\n",
    "\n",
    "    # In case all X coefficients are zero, then regress Y on D\n",
    "    if sum(SX_IDs) == 0 : \n",
    "        Naive[ i ] = sm.OLS( Y , sm.add_constant(D) ).fit().summary2().tables[1].round(3).iloc[ 1, 0 ] \n",
    "\n",
    "    # Otherwise, then regress Y on X and D (but only in the selected coefficients)\n",
    "    elif sum( SX_IDs ) > 0 :\n",
    "        X_D = np.concatenate( ( D, X[:, SX_IDs ] ) , axis = 1 )\n",
    "        Naive[ i ] = sm.OLS( Y , sm.add_constant( X_D ) ).fit().summary2().tables[1].round(3).iloc[ 1, 0]\n",
    "        \n",
    "    # Regress residuals. \n",
    "    resY = hdmpy.rlasso( X , Y , post = False ).est[ 'residuals' ]\n",
    "    resD = hdmpy.rlasso( X , D , post = False ).est[ 'residuals' ]\n",
    "    Orthogonal[ i ] = sm.OLS( resY , sm.add_constant( resD ) ).fit().summary2().tables[1].round(3).iloc[ 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Orto_breaks = [-1.2, -1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2]\n",
    "Naive_breaks = [-0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create histogram\n",
    "fig, axs = plt.subplots(1, 1, sharex= True, tight_layout=True)\n",
    "\n",
    "N, bins, patches = axs.hist( Orthogonal - 5 , range = (-2, 2), density = True , bins = Orto_breaks)\n",
    "\n",
    "fracs = ((N**(1 / 5)) / N.max())\n",
    "norm = colors.Normalize(fracs.min(), fracs.max())\n",
    " \n",
    "for thisfrac, thispatch in zip(fracs, patches):\n",
    "    color = plt.cm.viridis(norm(thisfrac))\n",
    "    thispatch.set_facecolor(color)\n",
    "\n",
    "plt.title('Orthogonal', size=20)\n",
    "plt.xlabel(\"Difference of Orhtogonal - True\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAacAAAEYCAYAAAD4czk4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAXjElEQVR4nO3de7RkZX3m8e8jNGoGR9BuBtJcGiJmeckg2CKoiR11DLB0MDMkwhABL+ng0owuL2tInKDRMdG4ZGUQpUOUQQ3eQYdhNUEywYBRiA02N9sLGBx6aEOLCjJesPE3f+zdWhTndNXprnPO213fz1q1atfe7977t8+pOs/Zl3p3qgpJklrysMUuQJKkYYaTJKk5hpMkqTmGkySpOYaTJKk5hpMkqTmGk7QIknwuid/jkGZhOEm9JNU/vpXkEbO0ub1vs/tC1ydNE8NJeqgDgdfO8zpOAZ4wz+uQdlqxhwip0x9m+x5QwO7Ar1TVd4ba3A4cBCypqi0LXqQ0Jdxzkh7sh8DbgH8NvHncmZKcluSiJN9M8qMk9yb5xyS/N0v7B51zSnJSf7jwrFnaPzzJ95J8e/iQYj/vlf30HyfZkOS/Jnn4uPVLrTGcpId6L3Ab8AdJHj/mPOcCK4CrgL8EPka3h/XhJG8bY/5PA/cAJ89yPut4YC/gbwb32JJ8APgI8Djg4r7279IF7N96bkw7K8NJGlJVPwXOAJYA7xhztidX1VOr6rSq+qOqWg0cAvw9cEaS5SPW+WPg48A+wDEzNDm1f/7g1hFJTgNeRhdsj6+ql1fV66vqmcCfAquAV41Zv9QUw0maQVV9Cvgi8NtJnjVG+9tmGHc/3Z7M7sBzx1jt1uA5dXBkkn2B3wK+XFU3DUx6DbAFeFlV/WhoWW8D7gZOHmO9UnPc5Zdm93rgC8C7kxxV27h6KMmBwH+hC6EDgUcONdnmnhNAVX0hydeBFybZu6q+1086GdgNuGBgfb8EHAZ8B3htkpkW+RO8IlA7KcNJmkVVfTHJp4ATgN+lO+z2EEkOAf4J2Bu4Gvgs3fmjB+jOQ50KjHtxwgeBtwMn0p3Hop//p8BHB9rtDQRYxhwu3JB2Fh7Wk7btDLpg+PMke8zS5nXAY4GXV9WqqvrPVfUnVfUW4PI5ru/DwM/oD+0lORz4NWBtVW0eaHdP//zlqsq2HnNcv9QEw0nahv5c0vuAg4E/nKXZ4/rni2aY9uw5ru8Ouosonp7kV5nhQoi+3X3ALcCTkjxmLuuQdgaGkzTaW4HvA28C9pxh+u3986rBkUl+C3jFdqzvgv755cBJdBc2XDpDu7OAPYDzk+w1PDHJ3kmO2I71S4vOcJJGqKrvAn9Gd57nsTM0eR9wP/DJJBcm+Yska4HLgE9txyovBu6l60JpH+Aj/eXtw3Wd36/7eOC2JB9J8o4k5yW5Avg2sHo71i8tOsNJGs/Z/GIP6UGq6kbgN+mu7DsOeCVdDxP/AVgz1xX1l4V/ku57VjB0SG+o7auAF9Jd9v48uvNf/x54NPAuui8ESzsd+9aTJDXHPSdJUnMMJ0lScwwnSVJzDCdJUnMWrfuipUuX1ooVKxZr9ZKkRXDdddd9p6qWjWq3aOG0YsUK1q1bt1irlyQtgiTfGqedh/UkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzTGcJEnNGdlDRJJHAFcBD+/bf6qq3jzUJsB/p7vR2g+B06rq+smXK0k77mfffvxilzCrh+379cUuoQnjdF/0E+A5VXVfkiXA55NcVlXXDLQ5Fji0fzwdOLd/liRpzkYe1qvOff3LJf1j+Pa5xwMf6tteA+yVZL/JlipJmhZjnXNKsluS9cBdwBVVde1Qk+XAHQOvN/bjhpezOsm6JOs2b968nSVLknZ1Y4VTVT1QVU8B9geOTPLkoSaZabYZlnNeVa2sqpXLlo3sMV2SNKXmdLVeVX0f+BxwzNCkjcABA6/3B+7ckcIkSdNrZDglWZZkr374kcDzgK8ONbsEOCWdo4B7qmrTpIuVJE2Hca7W2w/4YJLd6MLsE1V1aZLTAapqDbCW7jLyW+kuJX/pPNUrSZoCI8Opqm4EDp9h/JqB4QJeNdnSJEnTyh4iJEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzRkZTkkOSHJlkg1JbknymhnarEpyT5L1/ePM+SlXkjQNdh+jzRbg9VV1fZJHAdcluaKqvjLU7uqqesHkS5QkTZuRe05Vtamqru+HfwBsAJbPd2GSpOk1p3NOSVYAhwPXzjD56CQ3JLksyZNmmX91knVJ1m3evHnu1UqSpsLY4ZRkT+Ai4LVVde/Q5OuBg6rqMOA9wGdmWkZVnVdVK6tq5bJly7azZEnSrm6scEqyhC6YLqyqi4enV9W9VXVfP7wWWJJk6UQrlSRNjXGu1gvwAWBDVZ01S5t9+3YkObJf7t2TLFSSND3GuVrvmcBLgJuSrO/H/TFwIEBVrQFOAF6ZZAvwI+DEqqrJlytJmgYjw6mqPg9kRJtzgHMmVZQkabrZQ4QkqTmGkySpOYaTJKk5hpMkqTmGkySpOYaTJKk5hpMkqTmGkySpOYaTJKk5hpMkqTmGkySpOYaTJKk5hpMkqTmGkySpOYaTJKk5hpMkqTmGkySpOYaTJKk5hpMkqTmGkySpOYaTJKk5hpMkqTmGkySpOYaTJKk5hpMkqTkjwynJAUmuTLIhyS1JXjNDmyQ5O8mtSW5McsT8lCtJmga7j9FmC/D6qro+yaOA65JcUVVfGWhzLHBo/3g6cG7/LEnSnI3cc6qqTVV1fT/8A2ADsHyo2fHAh6pzDbBXkv0mXq0kaSrM6ZxTkhXA4cC1Q5OWA3cMvN7IQwOMJKuTrEuybvPmzXMsVZI0LcYOpyR7AhcBr62qe4cnzzBLPWRE1XlVtbKqVi5btmxulUqSpsZY4ZRkCV0wXVhVF8/QZCNwwMDr/YE7d7w8SdI0GudqvQAfADZU1VmzNLsEOKW/au8o4J6q2jTBOiVJU2Scq/WeCbwEuCnJ+n7cHwMHAlTVGmAtcBxwK/BD4KUTr1SSNDVGhlNVfZ6ZzykNtingVZMqSpI03ewhQpLUHMNJktQcw0mS1BzDSZLUHMNJktQcw0mS1BzDSZLUHMNJktQcw0mS1BzDSZLUHMNJktQcw0mS1BzDSZLUHMNJktQcw0mS1BzDSZLUHMNJktQcw0mS1BzDSZLUHMNJktQcw0mS1BzDSZLUHMNJktQcw0mS1JyR4ZTk/CR3Jbl5lumrktyTZH3/OHPyZUqSpsnuY7S5ADgH+NA22lxdVS+YSEWSpKk3cs+pqq4CvrsAtUiSBEzunNPRSW5IclmSJ83WKMnqJOuSrNu8efOEVi1J2tVMIpyuBw6qqsOA9wCfma1hVZ1XVSurauWyZcsmsGpJ0q5oh8Opqu6tqvv64bXAkiRLd7gySdLU2uFwSrJvkvTDR/bLvHtHlytJml4jr9ZL8lFgFbA0yUbgzcASgKpaA5wAvDLJFuBHwIlVVfNWsSRplzcynKrqpBHTz6G71FySpImwhwhJUnMMJ0lScwwnSVJzDCdJUnPG6VtPkrRA3v/1X1/sEmb1isdfvWDrcs9JktQcw0mS1BzDSZLUHMNJktQcw0mS1BzDSZLUHMNJktQcw0mS1BzDSZLUHMNJktQcw0mS1BzDSZLUHMNJktQcw0mS1BzDSZLUHMNJktQcw0mS1BzDSZLUHMNJktQcw0mS1JyR4ZTk/CR3Jbl5lulJcnaSW5PcmOSIyZcpSZom4+w5XQAcs43pxwKH9o/VwLk7XpYkaZqNDKequgr47jaaHA98qDrXAHsl2W9SBUqSps8kzjktB+4YeL2xH/cQSVYnWZdk3ebNmyewaknSrmgS4ZQZxtVMDavqvKpaWVUrly1bNoFVS5J2RZMIp43AAQOv9wfunMByJUlTahLhdAlwSn/V3lHAPVW1aQLLlSRNqd1HNUjyUWAVsDTJRuDNwBKAqloDrAWOA24Ffgi8dL6KlSRNh5HhVFUnjZhewKsmVpEkaerZQ4QkqTmGkySpOYaTJKk5hpMkqTmGkySpOYaTJKk5hpMkqTmGkySpOYaTJKk5hpMkqTmGkySpOYaTJKk5hpMkqTmGkySpOYaTJKk5hpMkqTmGkySpOYaTJKk5hpMkqTmGkySpOYaTJKk5hpMkqTmGkySpOYaTJKk5Y4VTkmOSfC3JrUnOmGH6qiT3JFnfP86cfKmSpGmx+6gGSXYD3gv8O2Aj8KUkl1TVV4aaXl1VL5iHGiVJU2acPacjgVur6ptVdT/wMeD4+S1LkjTNRu45AcuBOwZebwSePkO7o5PcANwJvKGqbhlukGQ1sBrgwAMPnHu1knYqz73ydYtdwoyueMJiV6BRxtlzygzjauj19cBBVXUY8B7gMzMtqKrOq6qVVbVy2bJlcypUkjQ9xgmnjcABA6/3p9s7+rmqureq7uuH1wJLkiydWJWSpKkyTjh9CTg0ycFJ9gBOBC4ZbJBk3yTph4/sl3v3pIuVJE2HkeecqmpLklcDlwO7AedX1S1JTu+nrwFOAF6ZZAvwI+DEqho+9CdJ0ljGuSBi66G6tUPj1gwMnwOcM9nSJEnTyh4iJEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzdl9sQuQtGMOfs+7F7uEWR3y5MWuQDsr95wkSc0xnCRJzTGcJEnNGSuckhyT5GtJbk1yxgzTk+TsfvqNSY6YfKmSpGkxMpyS7Aa8FzgWeCJwUpInDjU7Fji0f6wGzp1wnZKkKTLOntORwK1V9c2quh/4GHD8UJvjgQ9V5xpgryT7TbhWSdKUGOdS8uXAHQOvNwJPH6PNcmDTYKMkq+n2rADuS/K1OVU7/5YC31nsInbAzl4/7PzbYP0Dbp/UguZm5DbstkCFbJ9vNPse+n0yTrNR9R80zkLGCaeZqqntaENVnQecN8Y6F0WSdVW1crHr2F47e/2w82+D9S++nX0brL8zzmG9jcABA6/3B+7cjjaSJI1lnHD6EnBokoOT7AGcCFwy1OYS4JT+qr2jgHuqatPwgiRJGsfIw3pVtSXJq4HL6Q7Vnl9VtyQ5vZ++BlgLHAfcCvwQeOn8lTyvmj3kOKadvX7Y+bfB+hffzr4N1g+k6iGnhiRJWlT2ECFJao7hJElqztSFU5LHJLkiyTf6571naXd7kpuSrE+ybq7zz5dx1p/kgCRXJtmQ5JYkrxmY9pYk/7ffrvVJjlugure7C6xR8y6EMeo/ua/7xiRfSHLYwLQZ30sLbYxtWJXknoH3xpnjzrsQxqj/jQO135zkgSSP6act+u8gyflJ7kpy8yzTW/8MjKp/sp+BqpqqB/AXwBn98BnAO2dpdzuwdHvnX8z6gf2AI/rhRwFfB57Yv34L8IYFrnk34DbgEGAP4Iat9Qy0OQ64jO47c0cB1447byP1PwPYux8+dmv923ovNbgNq4BLt2feFuofav9C4O8b+x38BnAEcPMs05v9DIxZ/0Q/A1O350TX1dIH++EPAi9a4Pl31Mj1V9Wmqrq+H/4BsIGux47FsiNdYI0z73wbWUNVfaGqvte/vIbuu34t2ZGf407xOxhyEvDRBalsTFV1FfDdbTRp+TMwsv5JfwamMZz+TfXfweqf95mlXQGfTXJdum6X5jr/fJnT+pOsAA4Hrh0Y/ep+1/v8BTosOVv3VuO0GWfe+TbXGl5O9x/wVrO9lxbSuNtwdJIbklyW5ElznHc+jV1Dkl8CjgEuGhjdwu9glJY/A3O1w5+BXfI27Un+Dth3hklvmsNinllVdybZB7giyVf7/xzm3YTqJ8medB/Q11bVvf3oc4G30b1Z3ga8G3jZ9lc7XikzjBu3C6yxusaaZ2PXkOQ36T6YzxoYvWjvpcHSZhg3vA3XAwdV1X39ucjP0N1pYKf6HdAd0vvHqhr8L7+F38EoLX8Gxjapz8AuGU5V9bzZpiX5lyT7VdWmfpf5rlmWcWf/fFeST9PtWl8FjDX/YtefZAldMF1YVRcPLPtfBtr8NXDp5Cqf1Y50gbXHGPPOt7G650ryb4H3A8dW1d1bx2/jvbSQRm7DwD8wVNXaJO9LsnSceRfAXGo4kaFDeo38DkZp+TMwlkl+BqbxsN4lwKn98KnA/xxukORfJXnU1mHg+cDN484/z8apP8AHgA1VddbQtMFbmfw2v9iu+bQjXWCNM+98G1lDkgOBi4GXVNXXB8Zv6720kMbZhn379w5JjqT7+3D3OPMugLFqSPJo4NkMfC4a+h2M0vJnYKSJfwYW+oqPxX4AjwX+N/CN/vkx/fhfBtb2w4fQXRFzA3AL8KZR8zdW/7PodvtvBNb3j+P6aR8GbuqnXQLst0B1H0d31eBtW3+ewOnA6f1w6G5qeVtf38ptzbsI75tR9b8f+N7Az3vdqPdSg9vw6r7GG+hOaD9jZ/od9K9PAz42NF8TvwO6vblNwE/p9pJevpN9BkbVP9HPgN0XSZKaM42H9SRJjTOcJEnNMZwkSc0xnCRJzTGcJEnNMZy0aNL1Gr0+Xc/pNyR5XZKH9dNWJjm7H354kr/r2744ya/386xP8sjF3Yq5SfI76XqLv3Jo/IokleQPB8adk+S0Ecs7Pckp81QuST7d/5xvzYN7LH/GfK1TAu+Eq0WU5L6q2rMf3gf4CF23M28eancUXe/rz+5fr6Hr8fh/jLme0L3XfzbRDdgOSf6WblseEk50/R/+gK7H6fuTnEP3XZELFrzQIUlW0fVm/4Kh8btX1ZZFKUq7NPec1ISqugtYTdcpbdLdW+jSPrT+BnhK/x/7HwC/C5yZ5EL4+X18vpSuM9s/7cet6PdQ3kfXZ9wBI9r9db839tmte2NJHtfvsd2Q5PokvzLb+oYlOSnd/WtuTvLOftyZdF+QXpPkXTPMtpnui9WnDk9I8vv9Om9IclG6zk233p/rDUmekOSfBtqvSHJjP/zUJP+QrtPNy/PgXkLmLMlpST6Z5H/Rdea5KsmlA9N/vsc36XVrehhOakZVfZPuPbnPwLi7gFcAV1fVU6rqr+h6tnhjVZ2c5Pl0nZMeCTwFeGqS3+hn/1W6WxAc3g/P1u5Q4L1V9STg+8B/7Mdf2I8/jO5eNZtGrA+AJL8MvBN4Tt/maUleVFVvBdYBJ1fVG2f5MbwDeH2S3YbGX1xVT+tr2UD37fzBn90GYI8kh/SjXgx8Il0fi+8BTqiqpwLnA2+fZd1zcTRwalU9Z7YG87huTYFdsuNX7dRm6oF5W57fP77cv96TLjz+D/Ct6u6LM6rdP1fV+n78dcCKdH2BLa+qTwNU1Y8B+nCaaTmDnVg+DfhcVW3u57mQ7kZtnxm1MVX1z/0e0H8amvTkJP8N2Ktf5+UzzP4Jur3Kd9CF04vpQvnJdD1BQ3fjuk2j6hjDFfXgXr9nMl/r1hQwnNSM/r/+B+h6Wn/CuLMBf97vUQ0uawXw/8Zs95OBUQ8Aj2T2kJxxOTO02RF/BnyKBwfeBcCLquqG/pDZqhnm+zjwySQXA1VV30jya8AtVXX0rMV2e2nX9S8vqaozZ2s7YPBnu4UHH4V5xNZFj1q3NBsP66kJSZYBa4Bzam5X6VwOvCzdvatIsrw/T7W97YCf3z5iY5IX9e0f3p/nGWc51wLPTrK0/8N/EvAP425QVX0V+AowePHBo+gOKy4BTp5lvtvowvVP6IIK4GvAsiRH9/UuyS9uIrh1vgf6Q6ZPGTOYhn0LeGL/M3o08Nxx1y3Nxj0nLaZHJlkPLKH77/vDwFnbnGNIVX02yROAL/aHju4Dfo/uj/Sc2w15CfBXSd5K1xPz72xjOT+/r1Z199r6I+BKur2HtVU111urvJ1fHDqELnCupQuCm+jCaiYfB94FHNzXcn+SE4Cz++DYHfhLut6hJ6Kq7kjyCbqe7r+xte6FWLd2XV5KLklqjof1JEnNMZwkSc0xnCRJzTGcJEnNMZwkSc0xnCRJzTGcJEnN+f900BWUm8Tu5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create histogram\n",
    "fig, axs = plt.subplots(1, 1, sharex= True, tight_layout=True)\n",
    "\n",
    "N, bins, patches = axs.hist( Naive - 5, range = (-2, 2), density = True , bins = Naive_breaks)\n",
    "\n",
    "fracs = ((N**(1 /5 )) / N.max())\n",
    "norm = colors.Normalize(fracs.min(), fracs.max())\n",
    " \n",
    "for thisfrac, thispatch in zip(fracs, patches):\n",
    "    color = plt.cm.viridis(norm(thisfrac))\n",
    "    thispatch.set_facecolor(color)\n",
    "\n",
    "plt.title('Naive', size=20)\n",
    "plt.xlabel(\"Difference of Naive - True\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Simulation design B=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "B = 1000\n",
    "Naive = np.zeros( B )\n",
    "Orthogonal = np.zeros( B )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range( 0, B ):\n",
    "    n = 100\n",
    "    p = 100\n",
    "    beta = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "    gamma = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "\n",
    "    mean = 0\n",
    "    sd = 1\n",
    "    X = np.random.normal( mean , sd, n * p ).reshape( n, p )\n",
    "\n",
    "    D = ( X @ gamma ) + np.random.normal( mean , sd, n ).reshape( n, 1 )/4 \n",
    "    \n",
    "    # DGP \n",
    "    Y = 5*D + ( X @ beta ) + np.random.normal( mean , sd, n ).reshape( n, 1 )\n",
    "    # single selection method\n",
    "    r_lasso_estimation = hdmpy.rlasso( np.concatenate( ( D , X ) , axis  =  1 ) , Y , post = True ) # Regress main equation by lasso\n",
    "\n",
    "    coef_array = r_lasso_estimation.est[ 'coefficients' ].iloc[ 2:, :].to_numpy()    # Get \"X\" coefficients \n",
    "\n",
    "    SX_IDs = np.where( coef_array != 0 )[0]\n",
    "\n",
    "    # In case all X coefficients are zero, then regress Y on D\n",
    "    if sum(SX_IDs) == 0 : \n",
    "        Naive[ i ] = sm.OLS( Y , sm.add_constant(D) ).fit().summary2().tables[1].round(3).iloc[ 1, 0 ] \n",
    "\n",
    "    # Otherwise, then regress Y on X and D (but only in the selected coefficients)\n",
    "    elif sum( SX_IDs ) > 0 :\n",
    "        X_D = np.concatenate( ( D, X[:, SX_IDs ] ) , axis = 1 )\n",
    "        Naive[ i ] = sm.OLS( Y , sm.add_constant( X_D ) ).fit().summary2().tables[1].round(3).iloc[ 1, 0]\n",
    "\n",
    "    # In both cases we save D coefficient\n",
    "        \n",
    "    # Regress residuals. \n",
    "    resY = hdmpy.rlasso( X , Y , post = False ).est[ 'residuals' ]\n",
    "    resD = hdmpy.rlasso( X , D , post = False ).est[ 'residuals' ]\n",
    "    Orthogonal[ i ] = sm.OLS( resY , sm.add_constant( resD ) ).fit().summary2().tables[1].round(3).iloc[ 1, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Orto_breaks = [-1.2, -1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2]\n",
    "Naive_breaks = [-0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram\n",
    "fig, axs = plt.subplots(1, 1, sharex= True, tight_layout=True)\n",
    "\n",
    "N, bins, patches = axs.hist( Orthogonal - 5 , range = (-2, 2), density = True , bins = Orto_breaks)\n",
    "\n",
    "fracs = ((N**(1 / 5)) / N.max())\n",
    "norm = colors.Normalize(fracs.min(), fracs.max())\n",
    " \n",
    "for thisfrac, thispatch in zip(fracs, patches):\n",
    "    color = plt.cm.viridis(norm(thisfrac))\n",
    "    thispatch.set_facecolor(color)\n",
    "\n",
    "plt.title('Orthogonal', size=20)\n",
    "plt.xlabel(\"Difference of Orhtogonal - True\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create\n",
    "fig, axs = plt.subplots(1, 1, sharex= True, tight_layout=True)\n",
    "\n",
    "# Create histogram\n",
    "N, bins, patches = axs.hist( Naive - 5, range = (-2, 2), density = True , bins = Naive_breaks)\n",
    "\n",
    "fracs = ((N**(11 /12 )) / N.max())\n",
    "norm = colors.Normalize(fracs.min(), fracs.max())\n",
    " \n",
    "for thisfrac, thispatch in zip(fracs, patches):\n",
    "    color = plt.cm.viridis(norm(thisfrac))\n",
    "    thispatch.set_facecolor(color)\n",
    "\n",
    "plt.title('Naive', size=20)\n",
    "plt.xlabel(\"Difference of Naive - True\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain what do you see in the distribution of the histograms. Where the orthogonal and Naive should be centered?\n",
    "In the orthogonal figure, the distribution is converging to 0 meanwhile in the naive figure we have that the graphic is skewed to the right. If we increase the number of trials, in the orthogonal case we have the data looks more like a normal distribution but in the naive the data tend more to the right. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why does this happen? Give an econometric explanation of this.\n",
    "Neyman Orthogonality\n",
    "\n",
    "$ D = \\vartheta_n\\alpha(\\eta^0) = 0 $ , $\\vartheta_nM(\\alpha, \\eta^0) = 0 $ \n",
    "\n",
    "Neyman's orthogonality holds that the estimated coefficient $(\\ alpha)$ of the treatment depends on the coefficients estimated via partialing-out. That is, $\\ alpha$ is insensitive to changes in the estimated coefficients via partialing-out.\n",
    "\n",
    "Taking into account the concept of orthogonality, the estimation of the treatment coefficient using a double lasso via partialling-out model generates an estimated coefficient with a lower standard error, since this is more similar to the population coefficient. This is because in the second step of the partialling-out algorithm the treatment (D) is cleaned of all possible effects of D. In this sense, the coefficient estimated via partialling-out closely resembles the population parameter and complies with Neyman's orthogonality. However, the naive method generates a less precise estimated coefficient than the previous method, because although it uses laso and ols regression, it does not clean up the effects of X on D. This difference in the estimated coefficients can be observed in the histogram distributions for the following trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double-Lasso Approach\n",
    "We have some problems running regressions with many candidate predictors. Finding a model that helps us make decisions about which variables will be interesting and which ones is not a delicate task. Therefore, the main objective is to find a suitable model.\n",
    "\n",
    "One of them is related to DOUBLE-LASSO models, its method combines a regression model with a process of contractions of some parameters to zero and imposes a restriction or penalty on the regression coefficients, to select variables by two step, finding those that predict the dependent variable and those relative to independent variable. This is important, because exclusion of a covariate that is a modest predictor of the dependent variable but strong predictor of the independent variable can create a substantial omitted variable bias.Also, it helps to make test of randomization.\n",
    "\n",
    "In the following lines, the explanation about Lasso's steps will be explained according to the article \"Using double-loop regression for the selection of principle variables\" by Urminsky, Hansen and Chernozhukov,\n",
    "\n",
    "First, they estimate a linear regression model, because they want to find parameters that minimize the sum of squared errors in the regression equation:\n",
    "\n",
    "$$\n",
    " Y_i= \\beta_{0} + \\beta_{1}X_i +  \\beta_{2}W_i1+...+ \\beta_{k+1}W_ik+e_{i}\\\n",
    "$$\n",
    "\n",
    "A lasso regression find parameters which minimize the sum of squared errors in the regression equation with an additional penalty term:\n",
    "\n",
    "$$\n",
    "Min [\\sum_{i}(Y_i - \\beta_{0} + \\beta_{1}X_i +  \\beta_{2}W_i1+...+ \\beta_{k+1}W_ik)^2 + \\lambda \\sum_{k}|\\beta_{k}|\n",
    "$$\n",
    "\n",
    "The penalty term helps reduce the estimated regression coefficients to zero, and some are exactly zero, reducing the over-fitting. By setting some coefficients to zero, you also perform variable selection. Loop regression can be used when the number of observations is smaller than the number of predictors. However, we could omit important variables with nonzero coefficients and underestimate some nonzero coefficients. It would be a bias in the model. In this case, it is necessary to use the \"double lasso\" to select variables and alleviate both sources of bias.\n",
    "\n",
    "Step 1:Fit a lasso regression predicting the dependent variable, and keeping track of the variables with non-zero estimated coefficients:\n",
    "\n",
    "$$\n",
    "Y_i= \\alpha_{0} + \\alpha_{1}W_i1 +...+ \\alpha_{k}W_ik+e_{i}\\\n",
    "$$\n",
    "\n",
    "Step 2: Fit a lasso regression predicting the focal independent variable, keeping track of the variables with non-zero estimated coefficients:\n",
    "\n",
    "$$\n",
    "X_i= \\phi_{0} + \\phi_{1}W_i1 +...+ \\phi_{k}W_ik+e_{i}\\\n",
    "$$\n",
    "\n",
    "If Xi is an effectively randomized treatment, no covariates should be selected in this step.\n",
    "\n",
    "Step 3: Fit a linear regression of the dependent variable on the focal independent variable, including the covariates (Wik) selected in either of the first two steps\n",
    "\n",
    "$$\n",
    "Y_i= \\alpha_{0} + \\alpha_{1}W_i1 + \\sum_{k \\in A} \\beta_{k+1}+...+ \\alpha_{k}W_ik+e_{i}\\\n",
    "$$\n",
    "\n",
    "In the equation, A is the union of the variables estimated to have non-zero coefficients in Steps 1 and 2. This regression could also include a small set of additional covariates identified a priori as necessary. Interpret and report the coefficient estimates and significance tests on the focal variable(s) as the final results. So, lambda is important to avoid over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Convergence Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide an additional empirical example of partialling-out with Lasso to estimate the regression coefficient $\\beta_1$ in the high-dimensional linear regression model:\n",
    "  $$\n",
    "  Y = \\beta_1 D +  \\beta_2'W + \\epsilon.\n",
    "  $$\n",
    "  \n",
    "Specifically, we are interested in how the rates  at which economies of different countries grow ($Y$) are related to the initial wealth levels in each country ($D$) controlling for country's institutional, educational, and other similar characteristics ($W$).\n",
    "  \n",
    "The relationship is captured by $\\beta_1$, the *speed of convergence/divergence*, which measures the speed at which poor countries catch up $(\\beta_1< 0)$ or fall behind $(\\beta_1> 0)$ rich countries, after controlling for $W$. Our inference question here is: do poor countries grow faster than rich countries, controlling for educational and other characteristics? In other words, is the speed of convergence negative: $ \\beta_1 <0?$ This is the Convergence Hypothesis predicted by the Solow Growth Model. This is a structural economic model. Under some strong assumptions, that we won't state here, the predictive exercise we are doing here can be given causal interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outcome $Y$ is the realized annual growth rate of a country's wealth  (Gross Domestic Product per capita). The target regressor ($D$) is the initial level of the country's wealth. The target parameter $\\beta_1$ is the speed of convergence, which measures the speed at which poor countries catch up with rich countries. The controls ($W$) include measures of education levels, quality of institutions, trade openness, and political stability in the country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the data set GrowthData which is included in the package *hdm*. First, let us load the data set to get familiar with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdmpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I downloaded the data that the author used\n",
    "growth_read = pyreadr.read_r(\"C:/Users/edfra/OneDrive/Documentos/GitHub/ECO224/Labs/data/GrowthData.RData\")\n",
    "\n",
    "# Extracting the data frame from rdata_read\n",
    "growth = growth_read[ 'GrowthData' ]\n",
    "list(growth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We determine the dimension of our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025013,
     "end_time": "2021-01-20T08:46:45.109042",
     "exception": false,
     "start_time": "2021-01-20T08:46:45.084029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The sample contains $90$ countries and $63$ controls. Thus $p \\approx 60$, $n=90$ and $p/n$ is not small. We expect the least squares method to provide a poor estimate of $\\beta_1$.  We expect the method based on partialling-out with Lasso to provide a high quality estimate of $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024124,
     "end_time": "2021-01-20T08:46:45.157510",
     "exception": false,
     "start_time": "2021-01-20T08:46:45.133386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To check this hypothesis, we analyze the relation between the output variable $Y$ and the other country's characteristics by running a linear regression in the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the main variables\n",
    "y = growth['Outcome']\n",
    "X = growth.drop('Outcome', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS regression\n",
    "reg_ols  = sm.OLS(y, X).fit()\n",
    "print(reg_ols.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output: estimated regression coefficient corresponding to the target regressor\n",
    "est_ols = reg_ols.summary2().tables[1]['Coef.']['gdpsh465']\n",
    "\n",
    "# output: std. error\n",
    "std_ols = reg_ols.summary2().tables[1]['Std.Err.']['gdpsh465']\n",
    "\n",
    "# output: 95% confidence interval\n",
    "lower_ci = reg_ols.summary2().tables[1]['[0.025']['gdpsh465']\n",
    "upper_ci = reg_ols.summary2().tables[1]['0.975]']['gdpsh465']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize OLS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_1 = np.zeros( (1, 4) )\n",
    "\n",
    "table_1[0,0] = est_ols  \n",
    "table_1[0,1] = std_ols   \n",
    "table_1[0,2] = lower_ci\n",
    "table_1[0,3] = upper_ci    \n",
    "\n",
    "\n",
    "table_1_pandas = pd.DataFrame( table_1, columns = [ \"Estimator\",\"Std. Error\", \"lower bound CI\", \"upper bound CI\"  ])\n",
    "table_1_pandas.index = [ \"OLS\" ]\n",
    "table_1_html = table_1_pandas.to_html()\n",
    "table_1_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- html table generated in R 3.6.3 by xtable 1.8-4 package -->\n",
    "<!-- Tue Jan 19 10:23:32 2021 -->\n",
    "<table border=1>\n",
    "<tr> <th>  </th> <th> estimator </th> <th> standard error </th> <th> lower bound CI </th> <th> upper bound CI </th>  </tr>\n",
    "  <tr> <td align=\"right\"> OLS </td> <td align=\"right\"> -0.009 </td> <td align=\"right\"> 0.030 </td> <td align=\"right\"> -0.071 </td> <td align=\"right\"> 0.052 </td> </tr>\n",
    "   </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least squares provides a rather noisy estimate (high standard error) of the\n",
    "speed of convergence, and does not allow us to answer the question\n",
    "about the convergence hypothesis since the confidence interval includes zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create main variables\n",
    "Y = growth['Outcome']\n",
    "W = growth.drop(['Outcome','intercept', 'gdpsh465'], 1 )\n",
    "D = growth['gdpsh465']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double LASSO using cross Validation (Using Sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to use sample splitting approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "D_train, D_test, Y_train, Y_test, W_train, W_test = train_test_split(D, Y, W, test_size = 0.30)\n",
    "W_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rom sklearn import model_selection\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#kfold = model_selection.KFold(n_splits=10)\n",
    "#lr = LogisticRegression()\n",
    "#scoring = 'accuracy'\n",
    "#results = model_selection.cross_val_score(lr, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "#print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))\n",
    "#results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all, let's use the $ \\alpha $ = 0.00077 because it is the value that get answers pretty similar in comparison with R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Seat values for Lasso\n",
    "lasso_model = linear_model.Lasso( alpha = 0.00077 )\n",
    "r_Y = Y_train - lasso_model.fit(W_train,Y_train).predict(W_train)\n",
    "r_Y = r_Y.rename('r_Y')\n",
    "\n",
    "# Part. out d\n",
    "r_D = D_train - lasso_model.fit(W_train,D_train).predict(W_train)\n",
    "r_D = r_D.rename('r_D')\n",
    "\n",
    "# ols \n",
    "partial_lasso_fit = sm.OLS(r_Y, r_D).fit()\n",
    "\n",
    "est_lasso = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "std_lasso = partial_lasso_fit.summary2().tables[1]['Std.Err.']['r_D']\n",
    "lower_ci_lasso = partial_lasso_fit.summary2().tables[1]['[0.025']['r_D']\n",
    "upper_ci_lasso = partial_lasso_fit.summary2().tables[1]['0.975]']['r_D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import linear_model\n",
    "\n",
    "# # Seat values for Lasso\n",
    "# lasso_model = linear_model.Lasso( alpha = 0.00077 )\n",
    "# r_Y = Y - lasso_model.fit( W, Y ).predict( W )\n",
    "# r_Y = r_Y.rename('r_Y')\n",
    "\n",
    "# # Part. out d\n",
    "# r_D = D - lasso_model.fit( W, D ).predict( W )\n",
    "# r_D = r_D.rename('r_D')\n",
    "\n",
    "# # Regress residuales\n",
    "# partial_lasso_fit = sm.OLS(r_Y, r_D).fit()\n",
    "\n",
    "# # output: estimated regression coefficient corresponding to the target regressor\n",
    "# est_lasso = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "\n",
    "# # output: std. error\n",
    "# std_lasso = partial_lasso_fit.summary2().tables[1]['Std.Err.']['r_D']\n",
    "\n",
    "# # output: 95% confidence interval\n",
    "# lower_ci_lasso = partial_lasso_fit.summary2().tables[1]['[0.025']['r_D']\n",
    "# upper_ci_lasso = partial_lasso_fit.summary2().tables[1]['0.975]']['r_D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress residuales\n",
    "partial_lasso_fit = sm.OLS(r_Y, r_D).fit()\n",
    "partial_lasso_est = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "\n",
    "print( f\"Coefficient for D via sample splitting using lasso {partial_lasso_est}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output: estimated regression coefficient corresponding to the target regressor\n",
    "est_lasso_train = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "\n",
    "# output: std. error\n",
    "std_lasso_train = partial_lasso_fit.summary2().tables[1]['Std.Err.']['r_D']\n",
    "\n",
    "# output: 95% confidence interval\n",
    "lower_ci_lasso_train = partial_lasso_fit.summary2().tables[1]['[0.025']['r_D']\n",
    "upper_ci_lasso_train = partial_lasso_fit.summary2().tables[1]['0.975]']['r_D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_Y = pd.DataFrame(r_Y)\n",
    "r_D = pd.DataFrame(r_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "mymodel = model.fit(r_Y, r_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K - fold CV\n",
    "# Importing cross_val score function\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 5-fold CV\n",
    "scores = cross_val_score(mymodel, W_train, D_train ,Y_train)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Seat values for Lasso\n",
    "lasso_model = linear_model.Lasso( alpha = 0.00077 )\n",
    "r_Y = Y_test - lasso_model.fit(W_test,Y_test ).predict(W_test)\n",
    "r_Y = r_Y.rename('r_Y')\n",
    "\n",
    "# Part. out d\n",
    "r_D = D_test - lasso_model.fit(W_test,D_test).predict(W_test)\n",
    "r_D = r_D.rename('r_D')\n",
    "\n",
    "# ols \n",
    "partial_lasso_fit = sm.OLS(r_Y, r_D).fit()\n",
    "\n",
    "est_lasso = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "std_lasso = partial_lasso_fit.summary2().tables[1]['Std.Err.']['r_D']\n",
    "lower_ci_lasso = partial_lasso_fit.summary2().tables[1]['[0.025']['r_D']\n",
    "upper_ci_lasso = partial_lasso_fit.summary2().tables[1]['0.975]']['r_D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_Y = pd.DataFrame(r_Y)\n",
    "r_D = pd.DataFrame(r_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "mymodel = model.fit(r_Y, r_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K - fold CV\n",
    "# Importing cross_val score function\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 5-fold CV\n",
    "pred = cross_val_score(mymodel, W_test, D_test,Y_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress residuales\n",
    "partial_lasso_fit = sm.OLS(r_Y, r_D).fit()\n",
    "partial_lasso_est = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "\n",
    "print( f\"Coefficient for D via sample splitting using lasso {partial_lasso_est}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output: estimated regression coefficient corresponding to the target regressor\n",
    "est_lasso_test = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "\n",
    "# output: std. error\n",
    "std_lasso_test = partial_lasso_fit.summary2().tables[1]['Std.Err.']['r_D']\n",
    "\n",
    "# output: 95% confidence interval\n",
    "lower_ci_lasso_test = partial_lasso_fit.summary2().tables[1]['[0.025']['r_D']\n",
    "upper_ci_lasso_test = partial_lasso_fit.summary2().tables[1]['0.975]']['r_D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secondly, let's use the $ \\alpha $ = 0.1  in order to know how $\\beta$ changes in comparison when we use $ \\alpha $ = 0.00077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Seat values for Lasso\n",
    "lasso_model = linear_model.Lasso( alpha = 0.1 )\n",
    "r_Y = Y_train - lasso_model.fit(W_train,Y_train).predict(W_train)\n",
    "r_Y = r_Y.rename('r_Y')\n",
    "\n",
    "# Part. out d\n",
    "r_D = D_train - lasso_model.fit(W_train,D_train).predict(W_train)\n",
    "r_D = r_D.rename('r_D')\n",
    "\n",
    "# ols \n",
    "partial_lasso_fit = sm.OLS(r_Y, r_D).fit()\n",
    "\n",
    "est_lasso = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "std_lasso = partial_lasso_fit.summary2().tables[1]['Std.Err.']['r_D']\n",
    "lower_ci_lasso = partial_lasso_fit.summary2().tables[1]['[0.025']['r_D']\n",
    "upper_ci_lasso = partial_lasso_fit.summary2().tables[1]['0.975]']['r_D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress residuales\n",
    "partial_lasso_fit = sm.OLS(r_Y, r_D).fit()\n",
    "partial_lasso_est = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "\n",
    "print( f\"Coefficient for D via sample splitting using lasso {partial_lasso_est}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output: estimated regression coefficient corresponding to the target regressor\n",
    "est_lasso_train_1 = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "\n",
    "# output: std. error\n",
    "std_lasso_train_1 = partial_lasso_fit.summary2().tables[1]['Std.Err.']['r_D']\n",
    "\n",
    "# output: 95% confidence interval\n",
    "lower_ci_lasso_train_1 = partial_lasso_fit.summary2().tables[1]['[0.025']['r_D']\n",
    "upper_ci_lasso_train_1 = partial_lasso_fit.summary2().tables[1]['0.975]']['r_D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_Y = pd.DataFrame(r_Y)\n",
    "r_D = pd.DataFrame(r_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "mymodel = model.fit(r_Y, r_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K - fold CV\n",
    "# Importing cross_val score function\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 5-fold CV\n",
    "scores = cross_val_score(mymodel, W_train, D_train ,Y_train)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Seat values for Lasso\n",
    "lasso_model = linear_model.Lasso( alpha = 0.1 )\n",
    "r_Y = Y_test - lasso_model.fit(W_test,Y_test ).predict(W_test)\n",
    "r_Y = r_Y.rename('r_Y')\n",
    "\n",
    "# Part. out d\n",
    "r_D = D_test - lasso_model.fit(W_test,D_test).predict(W_test)\n",
    "r_D = r_D.rename('r_D')\n",
    "\n",
    "# ols \n",
    "partial_lasso_fit = sm.OLS(r_Y, r_D).fit()\n",
    "\n",
    "est_lasso = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "std_lasso = partial_lasso_fit.summary2().tables[1]['Std.Err.']['r_D']\n",
    "lower_ci_lasso = partial_lasso_fit.summary2().tables[1]['[0.025']['r_D']\n",
    "upper_ci_lasso = partial_lasso_fit.summary2().tables[1]['0.975]']['r_D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_Y = pd.DataFrame(r_Y)\n",
    "r_D = pd.DataFrame(r_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "mymodel = model.fit(r_Y, r_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K - fold CV\n",
    "# Importing cross_val score function\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 5-fold CV\n",
    "pred = cross_val_score(mymodel, W_test, D_test,Y_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress residuales\n",
    "partial_lasso_fit = sm.OLS(r_Y, r_D).fit()\n",
    "partial_lasso_est = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "\n",
    "print( f\"Coefficient for D via sample splitting using lasso {partial_lasso_est}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output: estimated regression coefficient corresponding to the target regressor\n",
    "est_lasso_test_1 = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "\n",
    "# output: std. error\n",
    "std_lasso_test_1 = partial_lasso_fit.summary2().tables[1]['Std.Err.']['r_D']\n",
    "\n",
    "# output: 95% confidence interval\n",
    "lower_ci_lasso_test_1 = partial_lasso_fit.summary2().tables[1]['[0.025']['r_D']\n",
    "upper_ci_lasso_test_1 = partial_lasso_fit.summary2().tables[1]['0.975]']['r_D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thridly, let's use the $ \\alpha $ = 0.8 in order to know how $\\beta$ changes in comparison when we use $ \\alpha $ =0.00077 and the other $ \\alpha $ = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Seat values for Lasso\n",
    "lasso_model = linear_model.Lasso( alpha = 0.8 )\n",
    "r_Y = Y_train - lasso_model.fit(W_train,Y_train).predict(W_train)\n",
    "r_Y = r_Y.rename('r_Y')\n",
    "\n",
    "# Part. out d\n",
    "r_D = D_train - lasso_model.fit(W_train,D_train).predict(W_train)\n",
    "r_D = r_D.rename('r_D')\n",
    "\n",
    "# ols \n",
    "partial_lasso_fit = sm.OLS(r_Y, r_D).fit()\n",
    "\n",
    "est_lasso = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "std_lasso = partial_lasso_fit.summary2().tables[1]['Std.Err.']['r_D']\n",
    "lower_ci_lasso = partial_lasso_fit.summary2().tables[1]['[0.025']['r_D']\n",
    "upper_ci_lasso = partial_lasso_fit.summary2().tables[1]['0.975]']['r_D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress residuales\n",
    "partial_lasso_fit = sm.OLS(r_Y, r_D).fit()\n",
    "partial_lasso_est = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "\n",
    "print( f\"Coefficient for D via sample splitting using lasso {partial_lasso_est}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output: estimated regression coefficient corresponding to the target regressor\n",
    "est_lasso_train_2 = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "\n",
    "# output: std. error\n",
    "std_lasso_train_2 = partial_lasso_fit.summary2().tables[1]['Std.Err.']['r_D']\n",
    "\n",
    "# output: 95% confidence interval\n",
    "lower_ci_lasso_train_2 = partial_lasso_fit.summary2().tables[1]['[0.025']['r_D']\n",
    "upper_ci_lasso_train_2 = partial_lasso_fit.summary2().tables[1]['0.975]']['r_D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_Y = pd.DataFrame(r_Y)\n",
    "r_D = pd.DataFrame(r_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "mymodel = model.fit(r_Y, r_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K - fold CV\n",
    "# Importing cross_val score function\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 5-fold CV\n",
    "pred = cross_val_score(mymodel, W_test, D_test,Y_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress residuales\n",
    "partial_lasso_fit = sm.OLS(r_Y, r_D).fit()\n",
    "partial_lasso_est = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "\n",
    "print( f\"Coefficient for D via sample splitting using lasso {partial_lasso_est}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output: estimated regression coefficient corresponding to the target regressor\n",
    "est_lasso_test_2 = partial_lasso_fit.summary2().tables[1]['Coef.']['r_D']\n",
    "\n",
    "# output: std. error\n",
    "std_lasso_test_2 = partial_lasso_fit.summary2().tables[1]['Std.Err.']['r_D']\n",
    "\n",
    "# output: 95% confidence interval\n",
    "lower_ci_lasso_test_2 = partial_lasso_fit.summary2().tables[1]['[0.025']['r_D']\n",
    "upper_ci_lasso_test_2 = partial_lasso_fit.summary2().tables[1]['0.975]']['r_D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary LASSO results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us have a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2 = np.zeros( (1, 4) )\n",
    "\n",
    "table_2[0,0] = est_lasso_train\n",
    "table_2[0,1] = std_lasso_train\n",
    "table_2[0,2] = lower_ci_lasso_train\n",
    "table_2[0,3] = upper_ci_lasso_train\n",
    "\n",
    "\n",
    "table_2_pandas = pd.DataFrame( table_2, columns = [ \"Estimator\",\"Std. Error\", \"lower bound CI\", \"upper bound CI\"  ])\n",
    "table_2_pandas.index = [ \"LASSO TRAIN (alpha=0.00077)\" ]\n",
    "table_2_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_3 = table_1_pandas.append(table_2_pandas)\n",
    "table_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_4 = table_2 = np.zeros( (1, 4) )\n",
    "\n",
    "table_4[0,0] = est_lasso_test\n",
    "table_4[0,1] = std_lasso_test\n",
    "table_4[0,2] = lower_ci_lasso_test\n",
    "table_4[0,3] = upper_ci_lasso_test\n",
    "\n",
    "table_4_pandas = pd.DataFrame( table_4, columns = [ \"Estimator\",\"Std. Error\", \"lower bound CI\", \"upper bound CI\"  ])\n",
    "table_4_pandas.index = [ \"LASSO TEST (alpha= 0.00077)\" ]\n",
    "table_4_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_5 = table_3.append(table_4_pandas)\n",
    "table_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_6 = np.zeros( (1, 4) )\n",
    "\n",
    "table_6[0,0] = est_lasso_train_1\n",
    "table_6[0,1] = std_lasso_train_1\n",
    "table_6[0,2] = lower_ci_lasso_train_1\n",
    "table_6[0,3] = upper_ci_lasso_train_1\n",
    "\n",
    "\n",
    "table_6_pandas = pd.DataFrame( table_2, columns = [ \"Estimator\",\"Std. Error\", \"lower bound CI\", \"upper bound CI\"  ])\n",
    "table_6_pandas.index = [ \"LASSO TRAIN ( alpha =0.1)\" ]\n",
    "table_6_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_7 = table_5.append(table_6_pandas)\n",
    "table_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_8 = table_2 = np.zeros( (1, 4) )\n",
    "\n",
    "table_8[0,0] = est_lasso_test_1\n",
    "table_8[0,1] = std_lasso_test_1\n",
    "table_8[0,2] = lower_ci_lasso_test_1\n",
    "table_8[0,3] = upper_ci_lasso_test_1\n",
    "\n",
    "table_8_pandas = pd.DataFrame( table_4, columns = [ \"Estimator\",\"Std. Error\", \"lower bound CI\", \"upper bound CI\"  ])\n",
    "table_8_pandas.index = [ \"LASSO TEST ( alpha= 0.1 )\" ]\n",
    "table_8_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_9 = table_7.append(table_8_pandas)\n",
    "table_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_10 = np.zeros( (1, 4) )\n",
    "\n",
    "table_10[0,0] = est_lasso_train_2\n",
    "table_10[0,1] = std_lasso_train_2\n",
    "table_10[0,2] = lower_ci_lasso_train_2\n",
    "table_10[0,3] = upper_ci_lasso_train_2\n",
    "\n",
    "\n",
    "table_10_pandas = pd.DataFrame( table_2, columns = [ \"Estimator\",\"Std. Error\", \"lower bound CI\", \"upper bound CI\"  ])\n",
    "table_10_pandas.index = [ \"LASSO TRAIN (alpha=0.8)\" ]\n",
    "table_10_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_11 = table_9.append(table_10_pandas)\n",
    "table_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_12 = np.zeros( (1, 4) )\n",
    "\n",
    "table_12[0,0] = est_lasso_test_2\n",
    "table_12[0,1] = std_lasso_test_2\n",
    "table_12[0,2] = lower_ci_lasso_test_2\n",
    "table_12[0,3] = upper_ci_lasso_test_2\n",
    "\n",
    "\n",
    "table_12_pandas = pd.DataFrame( table_2, columns = [ \"Estimator\",\"Std. Error\", \"lower bound CI\", \"upper bound CI\"  ])\n",
    "table_12_pandas.index = [ \"LASSO TEST (alpha=0.8)\" ]\n",
    "table_12_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_13 = table_11.append(table_12_pandas)\n",
    "table_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_13_html = table_13.to_html()\n",
    "print(table_13_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least square method provides a rather noisy estimate of the speed of convergence. We can not answer the question if poor countries grow faster than rich countries. The least square method does not work when the ratio $p/n$ is large.\n",
    "\n",
    "First of all, these are the $\\beta$'s for Lasso when $\\alpha$ = 0.00077.\n",
    "\n",
    "On the one hand, for  sample splitting via Lasso provides a more precise estimate. \n",
    "The Lasso (Train sample) based point estimate is $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-8.14\\%,1.03\\%]$ only includes negative numbers. This empirical evidence does NOT support the convergence hypothesis.\n",
    "\n",
    "On the other hand, The Lasso (Test sample) based point estimate is $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-17.38\\%,-0.87\\%]$ only includes negative numbers. This empirical evidence does support the convergence hypothesis.\n",
    "\n",
    "Secondly, these are the $\\beta$'s for Lasso when $\\alpha$ = 0.1.\n",
    "\n",
    "On the one hand, for  sample splitting via Lasso provides a more precise estimate. \n",
    "The Lasso (Train sample) based point estimate is $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-17.38\\%,-0.87\\%]$ only includes negative numbers. This empirical evidence does support the convergence hypothesis.\n",
    "\n",
    "On the other hand, The Lasso (Test sample) based point estimate is $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-17.38\\%,-0.87\\%]$ only includes negative numbers. This empirical evidence does support the convergence hypothesis.\n",
    "\n",
    "First of all, these are the $\\beta$'s for Lasso when $\\alpha$ = 0.8\n",
    "\n",
    "On the one hand, for  sample splitting via Lasso provides a more precise estimate. \n",
    "The Lasso (Train sample) based point estimate is $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-8.06\\%,-0.12\\%]$ only includes negative numbers. This empirical evidence does support the convergence hypothesis.\n",
    "\n",
    "On the other hand, The Lasso (Test sample) based point estimate is $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-8.06\\%,-0.12\\%]$ only includes negative numbers. This empirical evidence does support the convergence hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Estimator</th>\n",
    "      <th>Std. Error</th>\n",
    "      <th>lower bound CI</th>\n",
    "      <th>upper bound CI</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>OLS</th>\n",
    "      <td>-0.009378</td>\n",
    "      <td>0.029888</td>\n",
    "      <td>-0.070600</td>\n",
    "      <td>0.051844</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>LASSO TRAIN (alpha=0.00077)</th>\n",
    "      <td>-0.035543</td>\n",
    "      <td>0.022953</td>\n",
    "      <td>-0.081426</td>\n",
    "      <td>0.010340</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>LASSO TEST (alpha= 0.00077)</th>\n",
    "      <td>-0.091273</td>\n",
    "      <td>0.040154</td>\n",
    "      <td>-0.173811</td>\n",
    "      <td>-0.008736</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>LASSO TRAIN ( alpha =0.1)</th>\n",
    "      <td>-0.091273</td>\n",
    "      <td>0.040154</td>\n",
    "      <td>-0.173811</td>\n",
    "      <td>-0.008736</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>LASSO TEST ( alpha= 0.1 )</th>\n",
    "      <td>-0.091273</td>\n",
    "      <td>0.040154</td>\n",
    "      <td>-0.173811</td>\n",
    "      <td>-0.008736</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>LASSO TRAIN (alpha=0.8)</th>\n",
    "      <td>-0.040951</td>\n",
    "      <td>0.019292</td>\n",
    "      <td>-0.080605</td>\n",
    "      <td>-0.001296</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>LASSO TEST (alpha=0.8)</th>\n",
    "      <td>-0.040951</td>\n",
    "      <td>0.019292</td>\n",
    "      <td>-0.080605</td>\n",
    "      <td>-0.001296</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double LASSO using theoretical Lambda (HDM package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_Y = hdmpy.rlasso( W, Y, post=True ).est['residuals']\n",
    "res_D = hdmpy.rlasso( W, D, post=True ).est['residuals']\n",
    "\n",
    "r_Y = pd.DataFrame(res_Y, columns=['r_Y'])\n",
    "r_D = pd.DataFrame(res_D, columns=['r_D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS regression\n",
    "reg_ols  = sm.OLS(r_Y, r_D).fit()\n",
    "print(reg_ols.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output: estimated regression coefficient corresponding to the target regressor\n",
    "est_lasso_HDM = reg_ols.summary2().tables[1]['Coef.']['r_D']\n",
    "\n",
    "# output: std. error\n",
    "std_lasso_HDM = reg_ols.summary2().tables[1]['Std.Err.']['r_D']\n",
    "\n",
    "# output: 95% confidence interval\n",
    "lower_ci_lasso_HDM = reg_ols.summary2().tables[1]['[0.025']['r_D']\n",
    "upper_ci_lasso_HDM = reg_ols.summary2().tables[1]['0.975]']['r_D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_3 = np.zeros( (1, 4) )\n",
    "\n",
    "table_3[0,0] = est_lasso_HDM\n",
    "table_3[0,1] = std_lasso_HDM\n",
    "table_3[0,2] = lower_ci_lasso_HDM \n",
    "table_3[0,3] = upper_ci_lasso_HDM\n",
    "\n",
    "\n",
    "table_3_pandas = pd.DataFrame( table_3, columns = [ \"Estimator\",\"Std. Error\", \"lower bound CI\", \"upper bound CI\"  ]) \n",
    "table_3_pandas.index = [ \"LASSO_HDM\" ]\n",
    "table_3_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using HDM package via lasso provides estimation with $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-7.73\\%,-2.22\\%]$ only includes negative numbers. This empirical evidence does support the convergence hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double LASSO using method = \"partialling out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_direct = hdmpy.rlassoEffect(x=W, y=Y, d=D, method=\"partialling out\")\n",
    "lasso_direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_lasso_part_out = lasso_direct[\"coefficients\"]\n",
    "std_lasso_part_out = lasso_direct[\"se\"]\n",
    "lower_ci_lasso_part_out = est_lasso - 1.96*std_lasso\n",
    "upper_ci_lasso_part_out = est_lasso + 1.96*std_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_4 = np.zeros( (1, 4) )\n",
    "\n",
    "table_4[0,0] = est_lasso_part_out\n",
    "table_4[0,1] = std_lasso_part_out\n",
    "table_4[0,2] = lower_ci_lasso_part_out\n",
    "table_4[0,3] = upper_ci_lasso_part_out\n",
    "\n",
    "\n",
    "table_4_pandas = pd.DataFrame( table_4, columns = [ \"Estimator\",\"Std. Error\", \"lower bound CI\", \"upper bound CI\"  ]) \n",
    "table_4_pandas.index = [ \"LASSO_partialling_out\" ]\n",
    "table_4_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partialling-out approach via lasso does not allow us to answer the question about the convergence hypothesis since the confidence interval includes zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_14 = table_13.append(table_3_pandas)\n",
    "table_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_15 = table_14.append(table_4_pandas)\n",
    "table_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame(table_15) \n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = list(coef_df.index.values)\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df['variables'] = variables\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = coef_df['Estimator'] - coef_df['lower bound CI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df['errors'] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define figure, axes, and plot\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    \n",
    "# Error bars for 95% confidence interval\n",
    "# Can increase capsize to add whiskers\n",
    "coef_df.plot(x='variables', y='Estimator', kind='bar',\n",
    "            ax=ax, color='none', fontsize=22, \n",
    "            ecolor='steelblue',capsize=0,\n",
    "            yerr='errors', legend=False)\n",
    "    \n",
    "# Set title & labels\n",
    "plt.title('Coefficients of Features w/ 95% Confidence Intervals',fontsize=30)\n",
    "ax.set_ylabel('Coefficients',fontsize=22)\n",
    "ax.set_xlabel('',fontsize=22)\n",
    "    \n",
    "# Coefficients\n",
    "ax.scatter(x=pd.np.arange(coef_df.shape[0]), \n",
    "         marker='o', s=80, \n",
    "         y=coef_df['Estimator'], color='steelblue')\n",
    "    \n",
    "# Line to define zero on the y-axis\n",
    "ax.axhline(y=0, linestyle='--', color='red', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This graph shows the comparison of the coefficients in the different model approaches: OLS, Lasso (with $\\alpha$ = 0.00077), Lasso (with $\\alpha$ = 0.1), Lasso (with $\\alpha$ = 0.8), Lasso with the HDM package and the Lasso by the \"partialling out\" method. These are the following inferences:\n",
    "\n",
    "\n",
    "- First of all, Ordinal Least squares provides a rather noisy estimate (high standard error) of the speed of convergence, and does not allow us to answer the question about the convergence hypothesis since the confidence interval includes zero.\n",
    "\n",
    "- Secondly, on the one hand, for  sample splitting via Lasso provides a more precise estimate. \n",
    "\n",
    "- The Lasso (Train sample) based point estimate is $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-8.14\\%,1.03\\%]$ only includes negative numbers. This empirical evidence does NOT support the convergence hypothesis.\n",
    "\n",
    "- On the other hand, The Lasso (Test sample) based point estimate is $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-17.38\\%,-0.87\\%]$ only includes negative numbers. This empirical evidence does support the convergence hypothesis.\n",
    "\n",
    "- Thirdly, these are the $\\beta$'s for Lasso when $\\alpha$ = 0.1. On the one hand, for  sample splitting via Lasso provides a more precise estimate. \n",
    "\n",
    "- The Lasso (Train sample) based point estimate is $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-17.38\\%,-0.87\\%]$ only includes negative numbers. This empirical evidence does support the convergence hypothesis.\n",
    "\n",
    "- On the other hand, The Lasso (Test sample) based point estimate is $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-17.38\\%,-0.87\\%]$ only includes negative numbers. This empirical evidence does support the convergence hypothesis.\n",
    "\n",
    "- Fourthly, these are the $\\beta$'s for Lasso when $\\alpha$ = 0.8\n",
    "\n",
    "- One the one hand, The Lasso (Train sample) based point estimate is $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-8.06\\%,-0.12\\%]$ only includes negative numbers. This empirical evidence does support the convergence hypothesis.\n",
    "\n",
    "- On the other hand, The Lasso (Test sample) based point estimate is $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-8.06\\%,-0.12\\%]$ only includes negative numbers. This empirical evidence does support the convergence hypothesis.\n",
    "\n",
    "- Fifthly, The Lasso via HDM package based point estimate is $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-7.73\\%,-2.22\\%]$ only includes negative numbers. This empirical evidence does support the convergence hypothesis.\n",
    "\n",
    "- Sixthly, The Lasso via partialling out method based point estimate is $-5\\%$ and the $95\\%$ confidence interval for the (annual) rate of convergence $[-8.05\\%,-0.94\\%]$ only includes negative numbers. This empirical evidence does support the convergence hypothesis.\n",
    "\n",
    "In conclusion, only lasso in almost all the methods used give us significant results (only not in the train sample with $\\alpha$=0.00077). In other words, according to this empirical research we can say that poor countries grow faster than rich countries."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
